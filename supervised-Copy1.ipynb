{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage import exposure\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "n_classes = 10\n",
    "#dimension of z or latent representation\n",
    "z_dimension = 2\n",
    "#dimension of X or data\n",
    "X_dimension = 784\n",
    "#dimension of label of data\n",
    "y_dimension = 10\n",
    "\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "VALID_BATCH_SIZE = 10000\n",
    "EPOCHS = 500\n",
    "N = 1000\n",
    "TINY_ERROR = 1e-8\n",
    "DATA_PATH = \"/floyd/input/skripsi_datasets_2/\"\n",
    "cuda = torch.device('cuda:0')\n",
    "\n",
    "training_reconstruction_loss = []\n",
    "training_generator_loss = []\n",
    "training_discriminator_loss = []\n",
    "training_generator_sample = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../home/Data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(28), transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../home/Data/mnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(28), transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "class Convolutional_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Convolutional_Net, self).__init__()\n",
    "        self.convolutional1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.convolutional2 = nn.Conv2d(20, 50, 5 , 1)\n",
    "        self.linear1 = nn.Linear(4*4*50, 500)\n",
    "        self.linear2 = nn.Linear(500, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.convolutional1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.convolutional2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    classifier = Convolutional_Net()\n",
    "    classifier.load_state_dict(torch.load(\"fashionmnist.pt\"))\n",
    "    classifier.cuda(cuda)\n",
    "    classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_net, self).__init__()\n",
    "        self.layer1 = nn.Linear(X_dimension, N)\n",
    "        self.layer2 = nn.Linear(N, N)\n",
    "        self.layer3 = nn.Linear(N, z_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.layer1(x), p=0.5, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.layer2(x), p=0.5, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder_net, self).__init__()\n",
    "        self.layer1 = nn.Linear(z_dimension + n_classes, N)\n",
    "        self.layer2 = nn.Linear(N, N)\n",
    "        self.layer3 = nn.Linear(N, X_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        s = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "class Discriminator_net_gauss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator_net_gauss, self).__init__()\n",
    "        self.layer1 = nn.Linear(z_dimension, N)\n",
    "        self.layer2 = nn.Linear(N, N)\n",
    "        self.layer3 = nn.Linear(N, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.layer1(x), p=0.5, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.layer2(x), p=0.5, training=self.training)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.layer3(x)\n",
    "\n",
    "    \n",
    "def free_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def frozen_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor\n",
    "def train_one_epoch(decoder, encoder, discriminator_gauss, decoder_optimizer, encoder_optimizer, generator_optimizer, discriminator_optimizer, data_loader):\n",
    "    #'''\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    discriminator_gauss.train()\n",
    "\n",
    "    reconstruction_loss = None\n",
    "    discriminator_loss =None\n",
    "    generator_loss = None\n",
    "    \n",
    "    for X, target in data_loader:\n",
    "        #X = X * 0.3081 + 0.1307\n",
    "        X.resize_(TRAIN_BATCH_SIZE, X_dimension)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(cuda), target.cuda(cuda)\n",
    "\n",
    "        # Init gradients\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        discriminator_gauss.zero_grad()\n",
    "\n",
    "\n",
    "        z = encoder(X)\n",
    "        \n",
    "        category = np.array(target.data.tolist())\n",
    "        category = np.eye(n_classes)[category].astype('float32')\n",
    "        category = torch.from_numpy(category)\n",
    "        z_category = Variable(category)\n",
    "        \n",
    "        if cuda:\n",
    "            z_category = z_category.cuda(cuda)\n",
    "\n",
    "        z_with_label = torch.cat((z_category, z), 1)\n",
    "\n",
    "        decoded_X = decoder(z_with_label)\n",
    "        compared_with_original = X.resize(TRAIN_BATCH_SIZE, X_dimension)\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        reconstruction_loss = 0.5 * mse_loss(decoded_X + TINY_ERROR, compared_with_original + TINY_ERROR)\n",
    "        \n",
    "        reconstruction_loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        discriminator_gauss.zero_grad()\n",
    "\n",
    "        # Discriminator\n",
    "        encoder.eval()\n",
    "        valid = Variable(torch.randn(TRAIN_BATCH_SIZE, z_dimension) * 5.)\n",
    "        if cuda:\n",
    "            valid = valid.cuda(cuda)\n",
    "\n",
    "        fake = encoder(X)\n",
    "\n",
    "        discriminator_real_gauss = discriminator_gauss(valid)\n",
    "        discriminator_fake_gauss = discriminator_gauss(fake)\n",
    "\n",
    "        discriminator_loss = 0.5 * (torch.mean((discriminator_real_gauss + TINY_ERROR - 1)**2) + torch.mean((discriminator_fake_gauss + TINY_ERROR)**2))\n",
    "\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        discriminator_gauss.zero_grad()\n",
    "\n",
    "        # Generator\n",
    "        encoder = encoder.train()\n",
    "        fake = encoder(X)\n",
    "\n",
    "        generator_fake_gauss = discriminator_gauss(fake)\n",
    "        generator_loss = 0.5 * torch.mean((generator_fake_gauss + TINY_ERROR - 1)**2)\n",
    "\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        discriminator_gauss.zero_grad()\n",
    "    #'''\n",
    "    '''\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    discriminator_gauss.train()\n",
    "    \n",
    "    reconstruction_loss = None\n",
    "    real_loss = None\n",
    "    fake_loss = None\n",
    "    generator_loss = None\n",
    "    #reconstruction_loss = None\n",
    "    \n",
    "    for X, target in train_loader:\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        discriminator_gauss.zero_grad()\n",
    "        \n",
    "        valid = Variable(Tensor(TRAIN_BATCH_SIZE, 1).fill_(0.9), requires_grad=False)\n",
    "        fake = Variable(Tensor(TRAIN_BATCH_SIZE, 1).fill_(0.1), requires_grad=False)\n",
    "        \n",
    "        X = X.resize(TRAIN_BATCH_SIZE, 784)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        \n",
    "        if is_cuda:\n",
    "            X, target = X.cuda(cuda), target.cuda(cuda)\n",
    "              \n",
    "        \n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        \n",
    "        frozen_params(decoder)\n",
    "        frozen_params(encoder)\n",
    "        free_params(discriminator_gauss)\n",
    "        \n",
    "        z_real = torch.randn(TRAIN_BATCH_SIZE, z_dimension) * 1.0\n",
    "        z_real = z_real.cuda(cuda)\n",
    "        real_value = discriminator_gauss(z_real)\n",
    "        \n",
    "        z_fake = encoder(X)\n",
    "                \n",
    "        fake_value = discriminator_gauss(z_fake)\n",
    "        \n",
    "        real_loss = 1.0 * mse_loss(real_value + TINY_ERROR, valid)\n",
    "        fake_loss = 1.0 * mse_loss(fake_value + TINY_ERROR, fake)\n",
    "        \n",
    "        real_loss.backward()\n",
    "        fake_loss.backward()\n",
    "        \n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        free_params(decoder)\n",
    "        free_params(encoder)\n",
    "        frozen_params(discriminator_gauss)\n",
    "        \n",
    "        z = encoder(X)\n",
    "        \n",
    "        \n",
    "        category = np.array(target.data.tolist())\n",
    "        category = np.eye(n_classes)[category].astype('float32')\n",
    "        category = torch.from_numpy(category)\n",
    "        z_category = Variable(category)\n",
    "        \n",
    "        if cuda:\n",
    "            z_category = z_category.cuda(cuda)\n",
    "\n",
    "        #z_category = z_category.resize(TRAIN_BATCH_SIZE, 1, n_classes)\n",
    "        z_with_label = torch.cat((z_category, z), 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_hat = decoder(z_with_label)\n",
    "        \n",
    "        z_2 = encoder(Variable(X.data))\n",
    "        z_2_dis = discriminator_gauss(z_2)\n",
    "        \n",
    "        x_hat_resize = x_hat.view(-1, 784)\n",
    "        X_resize = X.view(-1, 784)\n",
    "        \n",
    "        reconstruction_loss = mse_loss(x_hat_resize + TINY_ERROR, X_resize)\n",
    "        generator_loss = 1.0 * mse_loss(z_2_dis + TINY_ERROR, valid)\n",
    "        \n",
    "        reconstruction_loss.backward()\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    '''\n",
    "    return discriminator_loss, generator_loss, reconstruction_loss\n",
    "    #return real_loss + fake_loss, generator_loss, reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"Kaos\",\n",
    "                 1: \"Cln Pnjg\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Gaun\",\n",
    "                 4: \"Mantel\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Kemeja\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Tas\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]\n",
    "\n",
    "def train_model(train_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        encoder = Encoder_net().cuda(cuda)\n",
    "        decoder = Decoder_net().cuda(cuda)\n",
    "        discriminator_gauss = Discriminator_net_gauss().cuda(cuda)\n",
    "    else:\n",
    "        encoder = Encoder_net()\n",
    "        decoder = Decoder_net()\n",
    "        discriminator_gauss = Discriminator_net_gauss()\n",
    "\n",
    "    #learning rates for optimization\n",
    "    learning_rate_1 = 0.0002\n",
    "    learning_rate_2 = 0.0002\n",
    "\n",
    "    #optimization for decoder and encoder\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate_1)\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate_1)\n",
    "\n",
    "    generator_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate_1)\n",
    "    discriminator_optimizer = optim.Adam(discriminator_gauss.parameters(), lr=learning_rate_1)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        discriminator_loss, generator_loss, reconstruction_loss = train_one_epoch(decoder, encoder, discriminator_gauss, \n",
    "                                                                              decoder_optimizer, encoder_optimizer, generator_optimizer, \n",
    "                                                                              discriminator_optimizer, train_loader)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        if epoch % 1 == 0:\n",
    "            training_reconstruction_loss.append(reconstruction_loss)\n",
    "            training_generator_loss.append(generator_loss)\n",
    "            training_discriminator_loss.append(discriminator_loss)\n",
    "            print('Epoch-{}, Time-{:.2}, Discriminator_loss-{:.4}, Generator_loss-{:.4}, reconstruction_loss-{:.4}'.format(epoch, epoch_time, discriminator_loss.item(), generator_loss.item(), reconstruction_loss.item()))\n",
    "    \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                encoder = encoder.eval()\n",
    "                decoder = decoder.eval()\n",
    "                discriminator_gauss = discriminator_gauss.eval()\n",
    "\n",
    "                X_test = None\n",
    "                y_test = None\n",
    "\n",
    "                for X, target in valid_loader:\n",
    "                    X_test = X\n",
    "                    y_test = target\n",
    "                    break\n",
    "\n",
    "                if is_cuda:\n",
    "                    X_test = X_test.cuda(cuda)\n",
    "\n",
    "                X_test = X_test.resize(VALID_BATCH_SIZE, X_dimension) \n",
    "\n",
    "                list_y_test = []\n",
    "                for item in y_test:\n",
    "                    list_y_test.append(item.item())\n",
    "\n",
    "\n",
    "                encoded_X_test = encoder(X_test)\n",
    "                training_generator_sample.append(encoded_X_test)\n",
    "                target_list = list_y_test\n",
    "\n",
    "                '''\n",
    "                figure = plt.figure()\n",
    "                set_classes = set(target_list)\n",
    "                color_map = plt.cm.rainbow(np.linspace(0, 1, len(set_classes)))\n",
    "                axis = plt.subplot(111, aspect='equal')\n",
    "                box = axis.get_position()\n",
    "                axis.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "                handles = [mpatches.Circle((0, 0), label=class_, color=color_map[i]) for i, class_ in enumerate(set_classes)]\n",
    "                axis.legend(handles=handles, shadow=True, bbox_to_anchor=(1.05, 0.45), fancybox=True, loc='center left')\n",
    "                kwargs = {'alpha': 0.8, 'c': [color_map[i] for i in target_list]}\n",
    "                encoded_X_test_cpu = encoded_X_test.cpu()\n",
    "                plt.scatter(encoded_X_test_cpu[:, 0].detach().numpy(), encoded_X_test_cpu[:, 1].detach().numpy(), s = 2, **kwargs)\n",
    "                axis.set_xlim([-20, 20])\n",
    "                axis.set_ylim([-20, 20])\n",
    "\n",
    "                plt.savefig('2_latent_space_supervised_aae_least/epoch_%d.png' % epoch)\n",
    "                plt.close('all')\n",
    "                '''\n",
    "\n",
    "                n_digits = 20\n",
    "                #decoded_X_test = decoder(encoder(X_test[:n_digits]))\n",
    "\n",
    "                category_test = np.array(y_test.numpy().data.tolist())\n",
    "                category_test = np.eye(n_classes)[category_test].astype('float32')\n",
    "                category_test = torch.from_numpy(category_test)\n",
    "                \n",
    "                z_category_test = Variable(category_test[:n_digits])\n",
    "                encoded_X_test = encoder(X_test[:n_digits])\n",
    "\n",
    "                if is_cuda:\n",
    "                    z_category_test = z_category_test.cuda(cuda)\n",
    "\n",
    "                encoded_X_test = torch.cat((z_category_test, encoded_X_test), 1)\n",
    "                decoded_X_test = decoder(encoded_X_test)\n",
    "\n",
    "                resized_decoded_X_test = decoded_X_test.resize(n_digits, 1, 28, 28)\n",
    "                resized_decoded_X_test = resized_decoded_X_test.cuda(cuda)\n",
    "                label_decoded = classifier(resized_decoded_X_test)\n",
    "                label_decoded = label_decoded.argmax(dim=1, keepdim=True)\n",
    "                label_decoded = torch.flatten(label_decoded)\n",
    "\n",
    "                original_X = X_test[:n_digits]\n",
    "                resized_original_X = original_X.resize(n_digits, 1, 28, 28)\n",
    "                resized_original_X = resized_original_X.cuda(cuda)\n",
    "                target_original_X = classifier(resized_original_X)\n",
    "                target_original_X = target_original_X.argmax(dim=1, keepdim=True)\n",
    "                target_original_X = torch.flatten(target_original_X)\n",
    "\n",
    "                decoded_label_cpu = label_decoded.cpu().detach().numpy()\n",
    "                decoded_target_original_X = target_original_X.cpu().detach().numpy()\n",
    "                decoded_X_test_cpu = decoded_X_test.cpu()\n",
    "                decoded_X_test_cpu = np.reshape(decoded_X_test_cpu.detach().numpy(), [-1, 28, 28]) * 255\n",
    "                figure = plt.figure(figsize=(20, 4))\n",
    "\n",
    "                for i in range (n_digits):\n",
    "                    axis = plt.subplot(2, n_digits, i + 1)\n",
    "                    axis.set_title(output_label(decoded_target_original_X[i]))\n",
    "                    X_test_cpu = X_test.cpu()\n",
    "                    plt.imshow(X_test_cpu[i].reshape(28, 28).detach().numpy())\n",
    "                    plt.gray()\n",
    "                    axis.get_xaxis().set_visible(False)\n",
    "                    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "                    axis = plt.subplot(2, n_digits, i + 1 + n_digits)\n",
    "                    axis.set_title(output_label(decoded_label_cpu[i]))\n",
    "                    plt.imshow(decoded_X_test_cpu[i])\n",
    "                    plt.gray()\n",
    "                    axis.get_xaxis().set_visible(False)\n",
    "                    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "                plt.savefig('4_reconstruction_supervised_aae_least/recon_%d.png' % epoch)\n",
    "                plt.close('all')\n",
    "                \n",
    "                \n",
    "                z = torch.randn(20, z_dimension) * 5\n",
    "                z = z.float().cuda(cuda)\n",
    "\n",
    "                target = torch.randint(10, (20, 1))\n",
    "                target = target.flatten()\n",
    "\n",
    "                one_hot_target = torch.zeros(20, 10)\n",
    "                one_hot_target[torch.arange(20), target] = 1\n",
    "                one_hot_target = one_hot_target.cuda(cuda)\n",
    "\n",
    "                z_target = torch.cat([one_hot_target, z], dim=1)\n",
    "                z_target = z_target.cuda(cuda)\n",
    "\n",
    "                recon_z = decoder(z_target)\n",
    "                recon_z = recon_z.resize(20, 1, 28, 28)\n",
    "                \n",
    "                recon = recon_z.cpu()\n",
    "                recon = np.reshape(recon.detach().numpy(), [-1, 28, 28]) * 255\n",
    "                \n",
    "                figure = plt.figure(figsize=(20, 4))\n",
    "\n",
    "                for i in range (20):\n",
    "                    axis = plt.subplot(2, n_digits, i + 1)\n",
    "                    plt.imshow(recon[i].reshape(28, 28))\n",
    "                    plt.gray()\n",
    "                    axis.get_xaxis().set_visible(False)\n",
    "                    axis.get_yaxis().set_visible(False)\n",
    "                \n",
    "                plt.savefig('4_sampling_supervised_aae_least/epoch_%d.png' % epoch)\n",
    "                plt.close()\n",
    "                \n",
    "\n",
    "                '''\n",
    "                z_sampling = [np.linspace(-5, 5, 10) for i in range (10)]\n",
    "\n",
    "                n_x, n_y = 10, 10\n",
    "                random_input = np.random.randn(10, z_dimension)\n",
    "                sample_y = np.identity(10)\n",
    "                plt.subplot()\n",
    "                grid_spec = gridspec.GridSpec(n_x, n_y, hspace=0.05, wspace=0.05)\n",
    "                i = 0\n",
    "                for r in random_input:\n",
    "                    for t in sample_y:\n",
    "                        r = np.reshape(r, (1, z_dimension))\n",
    "                        t = np.reshape(t, (1, n_classes))\n",
    "                        input_decoder = np.concatenate((t, r), 1)\n",
    "                        input_decoder = input_decoder.astype('float32')\n",
    "                        input_decoder = torch.from_numpy(input_decoder).float()\n",
    "                        input_decoder = input_decoder.cuda(cuda)\n",
    "\n",
    "                        decoded_X = decoder(input_decoder)\n",
    "                        decoded_X_cpu = decoded_X.cpu().detach().numpy()\n",
    "\n",
    "                        axis = plt.subplot(grid_spec[i])\n",
    "                        i += 1\n",
    "                        image = np.array(decoded_X_cpu.tolist()).reshape(28, 28)\n",
    "                        axis.imshow(image, cmap='gray')\n",
    "                        axis.set_xticks([])\n",
    "                        axis.set_yticks([])\n",
    "                        axis.set_aspect('auto')\n",
    "\n",
    "                plt.savefig('3_sampling_supervised_aae_least/epoch_%d.png' % epoch)\n",
    "                plt.close()\n",
    "                '''\n",
    "                \n",
    "                encoder = encoder.train()\n",
    "                decoder = decoder.train()\n",
    "                discriminator_gauss = discriminator_gauss.train()\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/opt/conda/lib/python3.6/site-packages/torch/tensor.py:357: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0, Time-1.3e+01, Discriminator_loss-0.2738, Generator_loss-0.1335, reconstruction_loss-0.01903\n",
      "Epoch-1, Time-1.2e+01, Discriminator_loss-0.2661, Generator_loss-0.1413, reconstruction_loss-0.01779\n",
      "Epoch-2, Time-1.2e+01, Discriminator_loss-0.2789, Generator_loss-0.1721, reconstruction_loss-0.01846\n",
      "Epoch-3, Time-1.2e+01, Discriminator_loss-0.2604, Generator_loss-0.1553, reconstruction_loss-0.01628\n",
      "Epoch-4, Time-1.2e+01, Discriminator_loss-0.2522, Generator_loss-0.1338, reconstruction_loss-0.01503\n",
      "Epoch-5, Time-1.2e+01, Discriminator_loss-0.2559, Generator_loss-0.1426, reconstruction_loss-0.01664\n",
      "Epoch-6, Time-1.3e+01, Discriminator_loss-0.2451, Generator_loss-0.1328, reconstruction_loss-0.01505\n",
      "Epoch-7, Time-1.2e+01, Discriminator_loss-0.2554, Generator_loss-0.1306, reconstruction_loss-0.01374\n",
      "Epoch-8, Time-1.2e+01, Discriminator_loss-0.247, Generator_loss-0.1353, reconstruction_loss-0.01524\n",
      "Epoch-9, Time-1.2e+01, Discriminator_loss-0.2488, Generator_loss-0.1255, reconstruction_loss-0.01489\n",
      "Epoch-10, Time-1.2e+01, Discriminator_loss-0.2494, Generator_loss-0.1268, reconstruction_loss-0.01397\n",
      "Epoch-11, Time-1.2e+01, Discriminator_loss-0.2517, Generator_loss-0.1293, reconstruction_loss-0.01333\n",
      "Epoch-12, Time-1.2e+01, Discriminator_loss-0.2493, Generator_loss-0.1253, reconstruction_loss-0.01412\n",
      "Epoch-13, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1274, reconstruction_loss-0.01665\n",
      "Epoch-14, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.1277, reconstruction_loss-0.01547\n",
      "Epoch-15, Time-1.2e+01, Discriminator_loss-0.2502, Generator_loss-0.1263, reconstruction_loss-0.01315\n",
      "Epoch-16, Time-1.3e+01, Discriminator_loss-0.2506, Generator_loss-0.1249, reconstruction_loss-0.01429\n",
      "Epoch-17, Time-1.5e+01, Discriminator_loss-0.2507, Generator_loss-0.1303, reconstruction_loss-0.01473\n",
      "Epoch-18, Time-1.3e+01, Discriminator_loss-0.2504, Generator_loss-0.1247, reconstruction_loss-0.01488\n",
      "Epoch-19, Time-1.2e+01, Discriminator_loss-0.2509, Generator_loss-0.1252, reconstruction_loss-0.01453\n",
      "Epoch-20, Time-1.2e+01, Discriminator_loss-0.2507, Generator_loss-0.1288, reconstruction_loss-0.01604\n",
      "Epoch-21, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1235, reconstruction_loss-0.01491\n",
      "Epoch-22, Time-1.4e+01, Discriminator_loss-0.2494, Generator_loss-0.1232, reconstruction_loss-0.01351\n",
      "Epoch-23, Time-1.3e+01, Discriminator_loss-0.2496, Generator_loss-0.1253, reconstruction_loss-0.0122\n",
      "Epoch-24, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.1245, reconstruction_loss-0.01483\n",
      "Epoch-25, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1254, reconstruction_loss-0.01317\n",
      "Epoch-26, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1248, reconstruction_loss-0.01444\n",
      "Epoch-27, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.1249, reconstruction_loss-0.01398\n",
      "Epoch-28, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01252\n",
      "Epoch-29, Time-1.2e+01, Discriminator_loss-0.2506, Generator_loss-0.1267, reconstruction_loss-0.01472\n",
      "Epoch-30, Time-1.3e+01, Discriminator_loss-0.2504, Generator_loss-0.1252, reconstruction_loss-0.01428\n",
      "Epoch-31, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01239\n",
      "Epoch-32, Time-1.2e+01, Discriminator_loss-0.2497, Generator_loss-0.125, reconstruction_loss-0.01317\n",
      "Epoch-33, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.0135\n",
      "Epoch-34, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1248, reconstruction_loss-0.01532\n",
      "Epoch-35, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1245, reconstruction_loss-0.0148\n",
      "Epoch-36, Time-1.2e+01, Discriminator_loss-0.2502, Generator_loss-0.1248, reconstruction_loss-0.01353\n",
      "Epoch-37, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01331\n",
      "Epoch-38, Time-1.4e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01436\n",
      "Epoch-39, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1252, reconstruction_loss-0.01342\n",
      "Epoch-40, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1255, reconstruction_loss-0.01283\n",
      "Epoch-41, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01444\n",
      "Epoch-42, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01305\n",
      "Epoch-43, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01433\n",
      "Epoch-44, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01315\n",
      "Epoch-45, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01426\n",
      "Epoch-46, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01312\n",
      "Epoch-47, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01311\n",
      "Epoch-48, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01338\n",
      "Epoch-49, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01384\n",
      "Epoch-50, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01379\n",
      "Epoch-51, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01301\n",
      "Epoch-52, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01372\n",
      "Epoch-53, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01453\n",
      "Epoch-54, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01232\n",
      "Epoch-55, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01275\n",
      "Epoch-56, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01299\n",
      "Epoch-57, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01389\n",
      "Epoch-58, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01401\n",
      "Epoch-59, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01488\n",
      "Epoch-60, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01371\n",
      "Epoch-61, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01235\n",
      "Epoch-62, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.012\n",
      "Epoch-63, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1252, reconstruction_loss-0.01382\n",
      "Epoch-64, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01301\n",
      "Epoch-65, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01373\n",
      "Epoch-66, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01365\n",
      "Epoch-67, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01356\n",
      "Epoch-68, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01292\n",
      "Epoch-69, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01451\n",
      "Epoch-70, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01334\n",
      "Epoch-71, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01573\n",
      "Epoch-72, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01326\n",
      "Epoch-73, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01396\n",
      "Epoch-74, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01358\n",
      "Epoch-75, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01393\n",
      "Epoch-76, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01384\n",
      "Epoch-77, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01363\n",
      "Epoch-78, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1252, reconstruction_loss-0.01313\n",
      "Epoch-79, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01103\n",
      "Epoch-80, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01334\n",
      "Epoch-81, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-82, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01354\n",
      "Epoch-83, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01495\n",
      "Epoch-84, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01291\n",
      "Epoch-85, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01324\n",
      "Epoch-86, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01256\n",
      "Epoch-87, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01317\n",
      "Epoch-88, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01361\n",
      "Epoch-89, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01361\n",
      "Epoch-90, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01566\n",
      "Epoch-91, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01357\n",
      "Epoch-92, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01354\n",
      "Epoch-93, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01502\n",
      "Epoch-94, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01341\n",
      "Epoch-95, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0135\n",
      "Epoch-96, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01368\n",
      "Epoch-97, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.0135\n",
      "Epoch-98, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.013\n",
      "Epoch-99, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01253\n",
      "Epoch-100, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01355\n",
      "Epoch-101, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01346\n",
      "Epoch-102, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01395\n",
      "Epoch-103, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01409\n",
      "Epoch-104, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01373\n",
      "Epoch-105, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01258\n",
      "Epoch-106, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01374\n",
      "Epoch-107, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01313\n",
      "Epoch-108, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0126\n",
      "Epoch-109, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01328\n",
      "Epoch-110, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01224\n",
      "Epoch-111, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01293\n",
      "Epoch-112, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01406\n",
      "Epoch-113, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01333\n",
      "Epoch-114, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0126\n",
      "Epoch-115, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01319\n",
      "Epoch-116, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01356\n",
      "Epoch-117, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01281\n",
      "Epoch-118, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01502\n",
      "Epoch-119, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01164\n",
      "Epoch-120, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01287\n",
      "Epoch-121, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01444\n",
      "Epoch-122, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01291\n",
      "Epoch-123, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01454\n",
      "Epoch-124, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01351\n",
      "Epoch-125, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01445\n",
      "Epoch-126, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01329\n",
      "Epoch-127, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01313\n",
      "Epoch-128, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01278\n",
      "Epoch-129, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01311\n",
      "Epoch-130, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01324\n",
      "Epoch-131, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01324\n",
      "Epoch-132, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01374\n",
      "Epoch-133, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01381\n",
      "Epoch-134, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01303\n",
      "Epoch-135, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01155\n",
      "Epoch-136, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01352\n",
      "Epoch-137, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01265\n",
      "Epoch-138, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01411\n",
      "Epoch-139, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0138\n",
      "Epoch-140, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01309\n",
      "Epoch-141, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01369\n",
      "Epoch-142, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01497\n",
      "Epoch-143, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01275\n",
      "Epoch-144, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01304\n",
      "Epoch-145, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01357\n",
      "Epoch-146, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01238\n",
      "Epoch-147, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01264\n",
      "Epoch-148, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01111\n",
      "Epoch-149, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01393\n",
      "Epoch-150, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01396\n",
      "Epoch-151, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01271\n",
      "Epoch-152, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01399\n",
      "Epoch-153, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01391\n",
      "Epoch-154, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.1249, reconstruction_loss-0.01326\n",
      "Epoch-155, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01298\n",
      "Epoch-156, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01404\n",
      "Epoch-157, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01291\n",
      "Epoch-158, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01249\n",
      "Epoch-159, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01279\n",
      "Epoch-160, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01481\n",
      "Epoch-161, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01375\n",
      "Epoch-162, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01368\n",
      "Epoch-163, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-164, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01368\n",
      "Epoch-165, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01241\n",
      "Epoch-166, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0127\n",
      "Epoch-167, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01222\n",
      "Epoch-168, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01378\n",
      "Epoch-169, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01428\n",
      "Epoch-170, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01259\n",
      "Epoch-171, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01471\n",
      "Epoch-172, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01225\n",
      "Epoch-173, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01226\n",
      "Epoch-174, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01342\n",
      "Epoch-175, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01264\n",
      "Epoch-176, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01277\n",
      "Epoch-177, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0131\n",
      "Epoch-178, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01356\n",
      "Epoch-179, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01313\n",
      "Epoch-180, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0132\n",
      "Epoch-181, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01273\n",
      "Epoch-182, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01268\n",
      "Epoch-183, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0136\n",
      "Epoch-184, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01308\n",
      "Epoch-185, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0139\n",
      "Epoch-186, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01225\n",
      "Epoch-187, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0135\n",
      "Epoch-188, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01274\n",
      "Epoch-189, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01324\n",
      "Epoch-190, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01277\n",
      "Epoch-191, Time-1.2e+01, Discriminator_loss-0.2507, Generator_loss-0.1257, reconstruction_loss-0.01378\n",
      "Epoch-192, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.1247, reconstruction_loss-0.01331\n",
      "Epoch-193, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01291\n",
      "Epoch-194, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01349\n",
      "Epoch-195, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0126\n",
      "Epoch-196, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01307\n",
      "Epoch-197, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01391\n",
      "Epoch-198, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01409\n",
      "Epoch-199, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01221\n",
      "Epoch-200, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01242\n",
      "Epoch-201, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01265\n",
      "Epoch-202, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01338\n",
      "Epoch-203, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01279\n",
      "Epoch-204, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01247\n",
      "Epoch-205, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01362\n",
      "Epoch-206, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01509\n",
      "Epoch-207, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01337\n",
      "Epoch-208, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01238\n",
      "Epoch-209, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01316\n",
      "Epoch-210, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01343\n",
      "Epoch-211, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01268\n",
      "Epoch-212, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01186\n",
      "Epoch-213, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.1245, reconstruction_loss-0.01334\n",
      "Epoch-214, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01455\n",
      "Epoch-215, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01327\n",
      "Epoch-216, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01468\n",
      "Epoch-217, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01259\n",
      "Epoch-218, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01209\n",
      "Epoch-219, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01377\n",
      "Epoch-220, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01397\n",
      "Epoch-221, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01216\n",
      "Epoch-222, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01159\n",
      "Epoch-223, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01379\n",
      "Epoch-224, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01377\n",
      "Epoch-225, Time-1.5e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01409\n",
      "Epoch-226, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.012\n",
      "Epoch-227, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01288\n",
      "Epoch-228, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01449\n",
      "Epoch-229, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01283\n",
      "Epoch-230, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01338\n",
      "Epoch-231, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01244\n",
      "Epoch-232, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01407\n",
      "Epoch-233, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01277\n",
      "Epoch-234, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01263\n",
      "Epoch-235, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01314\n",
      "Epoch-236, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01448\n",
      "Epoch-237, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01341\n",
      "Epoch-238, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01382\n",
      "Epoch-239, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01372\n",
      "Epoch-240, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01304\n",
      "Epoch-241, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01371\n",
      "Epoch-242, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01268\n",
      "Epoch-243, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01327\n",
      "Epoch-244, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01382\n",
      "Epoch-245, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-246, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01446\n",
      "Epoch-247, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01212\n",
      "Epoch-248, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01446\n",
      "Epoch-249, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01367\n",
      "Epoch-250, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1249, reconstruction_loss-0.01363\n",
      "Epoch-251, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0131\n",
      "Epoch-252, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01218\n",
      "Epoch-253, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01316\n",
      "Epoch-254, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01182\n",
      "Epoch-255, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01297\n",
      "Epoch-256, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01346\n",
      "Epoch-257, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01274\n",
      "Epoch-258, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01273\n",
      "Epoch-259, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01384\n",
      "Epoch-260, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01452\n",
      "Epoch-261, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01356\n",
      "Epoch-262, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01437\n",
      "Epoch-263, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01353\n",
      "Epoch-264, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01262\n",
      "Epoch-265, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01258\n",
      "Epoch-266, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1253, reconstruction_loss-0.01265\n",
      "Epoch-267, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01306\n",
      "Epoch-268, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01345\n",
      "Epoch-269, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01329\n",
      "Epoch-270, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01262\n",
      "Epoch-271, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01231\n",
      "Epoch-272, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01309\n",
      "Epoch-273, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0119\n",
      "Epoch-274, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01235\n",
      "Epoch-275, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01286\n",
      "Epoch-276, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01319\n",
      "Epoch-277, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01254\n",
      "Epoch-278, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01342\n",
      "Epoch-279, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01297\n",
      "Epoch-280, Time-1.5e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01341\n",
      "Epoch-281, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.0135\n",
      "Epoch-282, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01318\n",
      "Epoch-283, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01288\n",
      "Epoch-284, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01281\n",
      "Epoch-285, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01341\n",
      "Epoch-286, Time-1.3e+01, Discriminator_loss-0.2502, Generator_loss-0.1248, reconstruction_loss-0.01299\n",
      "Epoch-287, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01389\n",
      "Epoch-288, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01287\n",
      "Epoch-289, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01249\n",
      "Epoch-290, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01366\n",
      "Epoch-291, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0129\n",
      "Epoch-292, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01241\n",
      "Epoch-293, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01291\n",
      "Epoch-294, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01211\n",
      "Epoch-295, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01348\n",
      "Epoch-296, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01253\n",
      "Epoch-297, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01303\n",
      "Epoch-298, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01352\n",
      "Epoch-299, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0131\n",
      "Epoch-300, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01276\n",
      "Epoch-301, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01242\n",
      "Epoch-302, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01346\n",
      "Epoch-303, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01382\n",
      "Epoch-304, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01289\n",
      "Epoch-305, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01334\n",
      "Epoch-306, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1251, reconstruction_loss-0.01368\n",
      "Epoch-307, Time-1.3e+01, Discriminator_loss-0.2498, Generator_loss-0.1256, reconstruction_loss-0.0146\n",
      "Epoch-308, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01287\n",
      "Epoch-309, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01325\n",
      "Epoch-310, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01421\n",
      "Epoch-311, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01451\n",
      "Epoch-312, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01304\n",
      "Epoch-313, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01317\n",
      "Epoch-314, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01247\n",
      "Epoch-315, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01417\n",
      "Epoch-316, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01274\n",
      "Epoch-317, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01213\n",
      "Epoch-318, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01248\n",
      "Epoch-319, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01215\n",
      "Epoch-320, Time-1.5e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0135\n",
      "Epoch-321, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01325\n",
      "Epoch-322, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01246\n",
      "Epoch-323, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1263, reconstruction_loss-0.01406\n",
      "Epoch-324, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01426\n",
      "Epoch-325, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01272\n",
      "Epoch-326, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01228\n",
      "Epoch-327, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-328, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01313\n",
      "Epoch-329, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01209\n",
      "Epoch-330, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01344\n",
      "Epoch-331, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01234\n",
      "Epoch-332, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01355\n",
      "Epoch-333, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01343\n",
      "Epoch-334, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1247, reconstruction_loss-0.01384\n",
      "Epoch-335, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01347\n",
      "Epoch-336, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01353\n",
      "Epoch-337, Time-1.2e+01, Discriminator_loss-0.2498, Generator_loss-0.1249, reconstruction_loss-0.01198\n",
      "Epoch-338, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01346\n",
      "Epoch-339, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01352\n",
      "Epoch-340, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01213\n",
      "Epoch-341, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0124\n",
      "Epoch-342, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01403\n",
      "Epoch-343, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01248\n",
      "Epoch-344, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01356\n",
      "Epoch-345, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01208\n",
      "Epoch-346, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01249\n",
      "Epoch-347, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01186\n",
      "Epoch-348, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01267\n",
      "Epoch-349, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01197\n",
      "Epoch-350, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.1248, reconstruction_loss-0.01313\n",
      "Epoch-351, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01278\n",
      "Epoch-352, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01264\n",
      "Epoch-353, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.1249, reconstruction_loss-0.01373\n",
      "Epoch-354, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.013\n",
      "Epoch-355, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01319\n",
      "Epoch-356, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01484\n",
      "Epoch-357, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.013\n",
      "Epoch-358, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01336\n",
      "Epoch-359, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01343\n",
      "Epoch-360, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01316\n",
      "Epoch-361, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01405\n",
      "Epoch-362, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01184\n",
      "Epoch-363, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01446\n",
      "Epoch-364, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01332\n",
      "Epoch-365, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01427\n",
      "Epoch-366, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.0127\n",
      "Epoch-367, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01277\n",
      "Epoch-368, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0141\n",
      "Epoch-369, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01395\n",
      "Epoch-370, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01312\n",
      "Epoch-371, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.012\n",
      "Epoch-372, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01312\n",
      "Epoch-373, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01354\n",
      "Epoch-374, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01345\n",
      "Epoch-375, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0132\n",
      "Epoch-376, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0134\n",
      "Epoch-377, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01244\n",
      "Epoch-378, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01322\n",
      "Epoch-379, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01325\n",
      "Epoch-380, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01321\n",
      "Epoch-381, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01328\n",
      "Epoch-382, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01328\n",
      "Epoch-383, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01376\n",
      "Epoch-384, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01176\n",
      "Epoch-385, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01172\n",
      "Epoch-386, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01232\n",
      "Epoch-387, Time-1.2e+01, Discriminator_loss-0.2502, Generator_loss-0.125, reconstruction_loss-0.01281\n",
      "Epoch-388, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.014\n",
      "Epoch-389, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01295\n",
      "Epoch-390, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01227\n",
      "Epoch-391, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01279\n",
      "Epoch-392, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01325\n",
      "Epoch-393, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01389\n",
      "Epoch-394, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01337\n",
      "Epoch-395, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1253, reconstruction_loss-0.01274\n",
      "Epoch-396, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01233\n",
      "Epoch-397, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01344\n",
      "Epoch-398, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01232\n",
      "Epoch-399, Time-1.2e+01, Discriminator_loss-0.2502, Generator_loss-0.1247, reconstruction_loss-0.01318\n",
      "Epoch-400, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01241\n",
      "Epoch-401, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01289\n",
      "Epoch-402, Time-1.2e+01, Discriminator_loss-0.2498, Generator_loss-0.125, reconstruction_loss-0.01299\n",
      "Epoch-403, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01217\n",
      "Epoch-404, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0127\n",
      "Epoch-405, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0115\n",
      "Epoch-406, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01321\n",
      "Epoch-407, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01376\n",
      "Epoch-408, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01272\n",
      "Epoch-409, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-410, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01305\n",
      "Epoch-411, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01252\n",
      "Epoch-412, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01425\n",
      "Epoch-413, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01237\n",
      "Epoch-414, Time-1.2e+01, Discriminator_loss-0.2503, Generator_loss-0.1249, reconstruction_loss-0.01262\n",
      "Epoch-415, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01292\n",
      "Epoch-416, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.014\n",
      "Epoch-417, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01183\n",
      "Epoch-418, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0132\n",
      "Epoch-419, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01213\n",
      "Epoch-420, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01379\n",
      "Epoch-421, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01201\n",
      "Epoch-422, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01282\n",
      "Epoch-423, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01201\n",
      "Epoch-424, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01203\n",
      "Epoch-425, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01302\n",
      "Epoch-426, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01359\n",
      "Epoch-427, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01326\n",
      "Epoch-428, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01276\n",
      "Epoch-429, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01223\n",
      "Epoch-430, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01221\n",
      "Epoch-431, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.0142\n",
      "Epoch-432, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01324\n",
      "Epoch-433, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01248\n",
      "Epoch-434, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1253, reconstruction_loss-0.01276\n",
      "Epoch-435, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01258\n",
      "Epoch-436, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0118\n",
      "Epoch-437, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01292\n",
      "Epoch-438, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01281\n",
      "Epoch-439, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01375\n",
      "Epoch-440, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01288\n",
      "Epoch-441, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01234\n",
      "Epoch-442, Time-1.2e+01, Discriminator_loss-0.2507, Generator_loss-0.1246, reconstruction_loss-0.0124\n",
      "Epoch-443, Time-1.3e+01, Discriminator_loss-0.2494, Generator_loss-0.1249, reconstruction_loss-0.01275\n",
      "Epoch-444, Time-1.2e+01, Discriminator_loss-0.2503, Generator_loss-0.1249, reconstruction_loss-0.0138\n",
      "Epoch-445, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01361\n",
      "Epoch-446, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01161\n",
      "Epoch-447, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01276\n",
      "Epoch-448, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01374\n",
      "Epoch-449, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01421\n",
      "Epoch-450, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01373\n",
      "Epoch-451, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01476\n",
      "Epoch-452, Time-1.3e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01296\n",
      "Epoch-453, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01274\n",
      "Epoch-454, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01197\n",
      "Epoch-455, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01335\n",
      "Epoch-456, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01327\n",
      "Epoch-457, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0138\n",
      "Epoch-458, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01444\n",
      "Epoch-459, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01346\n",
      "Epoch-460, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01222\n",
      "Epoch-461, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01353\n",
      "Epoch-462, Time-1.2e+01, Discriminator_loss-0.2498, Generator_loss-0.125, reconstruction_loss-0.0136\n",
      "Epoch-463, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01193\n",
      "Epoch-464, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.1251, reconstruction_loss-0.0132\n",
      "Epoch-465, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01231\n",
      "Epoch-466, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01286\n",
      "Epoch-467, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01276\n",
      "Epoch-468, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01193\n",
      "Epoch-469, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01254\n",
      "Epoch-470, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01297\n",
      "Epoch-471, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01274\n",
      "Epoch-472, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01289\n",
      "Epoch-473, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01293\n",
      "Epoch-474, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01368\n",
      "Epoch-475, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01392\n",
      "Epoch-476, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01292\n",
      "Epoch-477, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01225\n",
      "Epoch-478, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01256\n",
      "Epoch-479, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.013\n",
      "Epoch-480, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01407\n",
      "Epoch-481, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01366\n",
      "Epoch-482, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.0141\n",
      "Epoch-483, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01298\n",
      "Epoch-484, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01337\n",
      "Epoch-485, Time-1.3e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01316\n",
      "Epoch-486, Time-1.2e+01, Discriminator_loss-0.2498, Generator_loss-0.1249, reconstruction_loss-0.0135\n",
      "Epoch-487, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01123\n",
      "Epoch-488, Time-1.3e+01, Discriminator_loss-0.2503, Generator_loss-0.1243, reconstruction_loss-0.01327\n",
      "Epoch-489, Time-1.2e+01, Discriminator_loss-0.2499, Generator_loss-0.125, reconstruction_loss-0.01247\n",
      "Epoch-490, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1251, reconstruction_loss-0.01169\n",
      "Epoch-491, Time-1.2e+01, Discriminator_loss-0.25, Generator_loss-0.1249, reconstruction_loss-0.01369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-492, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01279\n",
      "Epoch-493, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01378\n",
      "Epoch-494, Time-1.3e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01287\n",
      "Epoch-495, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01322\n",
      "Epoch-496, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01363\n",
      "Epoch-497, Time-1.4e+01, Discriminator_loss-0.25, Generator_loss-0.125, reconstruction_loss-0.01257\n",
      "Epoch-498, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01306\n",
      "Epoch-499, Time-1.2e+01, Discriminator_loss-0.2501, Generator_loss-0.125, reconstruction_loss-0.01438\n"
     ]
    }
   ],
   "source": [
    "trained_encoder, trained_decoder = train_model(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location_encoder = \"4-least-supervised-encoder.pt\"\n",
    "file_location_decoder = \"4-least-supervised-decoder.pt\"\n",
    "torch.save(trained_encoder.state_dict(), file_location_encoder)\n",
    "torch.save(trained_decoder.state_dict(), file_location_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_net(\n",
      "  (layer1): Linear(in_features=12, out_features=1000, bias=True)\n",
      "  (layer2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (layer3): Linear(in_features=1000, out_features=784, bias=True)\n",
      ")\n",
      "Encoder_net(\n",
      "  (layer1): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (layer2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (layer3): Linear(in_features=1000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoder = Encoder_net()\n",
    "    encoder.load_state_dict(torch.load(\"4-least-supervised-encoder.pt\"))\n",
    "    encoder.cuda(cuda)\n",
    "    encoder.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    decoder = Decoder_net()\n",
    "    decoder.load_state_dict(torch.load(\"4-least-supervised-decoder.pt\"))\n",
    "    decoder.cuda(cuda)\n",
    "    decoder.eval()\n",
    "\n",
    "print(decoder)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/tensor.py:357: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.02578\n",
      "tensor(8810, device='cuda:0')\n",
      "Accuracy : 88.1\n"
     ]
    }
   ],
   "source": [
    "for y in range(1):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "\n",
    "    for (x, target) in (valid_loader):\n",
    "        X_test = x\n",
    "        y_test = target\n",
    "        break\n",
    "        \n",
    "    if is_cuda:\n",
    "        X_test = X_test.cuda(cuda)\n",
    "        #y_test = y_test.cuda(cuda)\n",
    "\n",
    "  \n",
    "    \n",
    "    category_test = np.array(y_test.numpy().data.tolist())\n",
    "    category_test = np.eye(n_classes)[category_test].astype('float32')\n",
    "    category_test = torch.from_numpy(category_test)\n",
    "    z_category_test = Variable(category_test)\n",
    "    \n",
    "    z_category_test = z_category_test.cuda(cuda)\n",
    "    \n",
    "    \n",
    "\n",
    "    X_test = X_test.resize(VALID_BATCH_SIZE, 784)\n",
    "    \n",
    "    encoded_X_test = encoder(X_test)\n",
    "    encoded_X_test = torch.cat((z_category_test, encoded_X_test), 1)\n",
    "    decoded_X_test = decoder(encoded_X_test)\n",
    "\n",
    "    resized_decoded_X_test = decoded_X_test.resize(VALID_BATCH_SIZE, 1, 28, 28)\n",
    "    resized_decoded_X_test = resized_decoded_X_test.cuda(cuda)\n",
    "    \n",
    "    label_decoded = classifier(resized_decoded_X_test)\n",
    "    label_decoded = label_decoded.data.max(1, keepdim=True)[1]\n",
    "    label_decoded = torch.flatten(label_decoded)\n",
    "\n",
    "    #original_X = X_test[:n_digits]\n",
    "    \n",
    "    resized_original_X = X_test.resize(VALID_BATCH_SIZE, 1, 28, 28)\n",
    "    resized_original_X = resized_original_X.cuda(cuda)\n",
    "    \n",
    "    target_original_X = classifier(resized_original_X)\n",
    "    target_original_X = target_original_X.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    \n",
    "    #reconstruction error\n",
    "    X_test = X_test.resize(VALID_BATCH_SIZE, 784)\n",
    "    decoded_X_test = decoded_X_test.resize(VALID_BATCH_SIZE, 784)\n",
    "    \n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    rec_loss = mse_loss(X_test, decoded_X_test)\n",
    "    \n",
    "    print(\"MSE Loss: {:.4}\".format(rec_loss))\n",
    "    \n",
    "    #classification error\n",
    "    correct = label_decoded.eq(target_original_X.data.view_as(label_decoded)).sum()\n",
    "    accuracy = 100.0 * correct / VALID_BATCH_SIZE\n",
    "    print(correct)\n",
    "    print (\"Accuracy : {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
